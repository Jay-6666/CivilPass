import streamlit as st
import pandas as pd
from io import BytesIO
from datetime import datetime
from src.utils.oss import get_cached_oss_object, bucket
import math

def display_news():
    """æ˜¾ç¤ºæ”¿ç­–èµ„è®¯"""
    st.title("ğŸ“° æ”¿ç­–èµ„è®¯")
    st.caption("ğŸ“¢ æœ€æ–°å…¬åŠ¡å‘˜æ”¿ç­–åŠ¨æ€ä¸æƒå¨è§£è¯»")
    st.markdown("---")

    @st.cache_data(ttl=3600, show_spinner="æ­£åœ¨åŠ è½½æœ€æ–°æ”¿ç­–èµ„è®¯...")
    def load_all_policy_data():
        # é…ç½®å‚æ•°
        OSS_PATH = "æ”¿ç­–å’¨è¯¢"  # OSSå­˜å‚¨è·¯å¾„
        REQUIRED_COLUMNS = ["title", "source", "date", "url"]  # å¿…è¦å­—æ®µ
        DEFAULT_VALUES = {"summary": "æš‚æ— æ‘˜è¦", "region": "å…¨å›½", "hotness": 0}  # é»˜è®¤å€¼é…ç½®

        all_dfs = []
        error_files = []

        try:
            # è·å–ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
            files = bucket.list_objects(OSS_PATH).object_list
            csv_files = [f.key for f in files if f.key.endswith(".csv")]

            if not csv_files:
                st.error("âŒ ç›®å½•ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
                return pd.DataFrame()

            progress_text = f"æ­£åœ¨åŠ è½½ {len(csv_files)} ä¸ªæ•°æ®æº..."
            progress_bar = st.progress(0, text=progress_text)

            for i, file_path in enumerate(csv_files):
                try:
                    # è¯»å–CSVæ–‡ä»¶
                    csv_data = bucket.get_object(file_path).read()
                    df = pd.read_csv(BytesIO(csv_data), parse_dates=["date"])

                    # å­—æ®µæ ¡éªŒ
                    missing_cols = set(REQUIRED_COLUMNS) - set(df.columns)
                    if missing_cols:
                        raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µï¼š{', '.join(missing_cols)}")

                    # æ·»åŠ æ•°æ®æºæ ‡è¯†
                    df["data_source"] = file_path.split("/")[-1]

                    # è¡¥å……é»˜è®¤å€¼
                    for col, value in DEFAULT_VALUES.items():
                        df[col] = value

                    all_dfs.append(df)

                except Exception as e:
                    error_files.append((file_path, str(e)))
                finally:
                    progress_bar.progress((i + 1) / len(csv_files), text=progress_text)

            # åˆå¹¶æ•°æ®
            if not all_dfs:
                st.error("âŒ æ‰€æœ‰æ–‡ä»¶åŠ è½½å¤±è´¥")
                return pd.DataFrame()

            combined_df = pd.concat(all_dfs, ignore_index=True)

            # æ•°æ®æ¸…æ´—
            combined_df = (combined_df.dropna(subset=["title", "url"])
                         .drop_duplicates("url", keep="first")
                         .sort_values("date", ascending=False)
                         .reset_index(drop=True))

            return combined_df

        except Exception as e:
            st.error(f"âŒ ç›®å½•è®¿é—®å¤±è´¥ï¼š{str(e)}")
            return pd.DataFrame()

    df = load_all_policy_data()
    if df.empty:
        st.warning("âš ï¸ å½“å‰æ— å¯ç”¨æ”¿ç­–æ•°æ®")
        return

    with st.expander("ğŸ” æ™ºèƒ½ç­›é€‰", expanded=True):
        col1, col2 = st.columns([2, 3])
        with col1:
            date_range = st.date_input(
                "ğŸ“… æ—¥æœŸèŒƒå›´",
                value=(df["date"].min().date(), df["date"].max().date()),
                format="YYYY/MM/DD"
            )
            sources = st.multiselect(
                "ğŸ›ï¸ ä¿¡æ¯æ¥æº",
                options=df["source"].unique(),
                placeholder="å…¨éƒ¨æ¥æº"
            )
        with col2:
            keyword = st.text_input(
                "ğŸ” å…³é”®è¯æœç´¢",
                placeholder="æ ‡é¢˜/å†…å®¹å…³é”®è¯ï¼ˆæ”¯æŒç©ºæ ¼åˆ†éš”å¤šä¸ªå…³é”®è¯ï¼‰",
                help="ç¤ºä¾‹ï¼šå…¬åŠ¡å‘˜ å¾…é‡ è°ƒæ•´"
            )
            regions = st.multiselect(
                "ğŸŒ ç›¸å…³åœ°åŒº",
                options=df["region"].unique(),
                placeholder="å…¨å›½èŒƒå›´"
            )

    def process_data(df):
        start_date = pd.Timestamp(date_range[0])
        end_date = pd.Timestamp(date_range[1])
        filtered = df[df["date"].between(start_date, end_date)]
        
        if sources:
            filtered = filtered[filtered["source"].isin(sources)]
        if regions:
            filtered = filtered[filtered["region"].isin(regions)]
        if keyword:
            keywords = [k.strip() for k in keyword.split()]
            pattern = "|".join(keywords)
            filtered = filtered[
                filtered["title"].str.contains(pattern, case=False) |
                filtered["summary"].str.contains(pattern, case=False)
            ]
        
        return filtered.sort_values("date", ascending=False)

    processed_df = process_data(df)

    if processed_df.empty:
        st.warning("ğŸ˜¢ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èµ„è®¯")
    else:
        # åˆ†é¡µè®¾ç½®
        items_per_page = 30
        total_items = len(processed_df)
        total_pages = math.ceil(total_items / items_per_page)
        
        col1, col2, col3 = st.columns([2, 3, 2])
        with col2:
            current_page = st.number_input(
                "é¡µç ",
                min_value=1,
                max_value=total_pages,
                value=1,
                help=f"å…± {total_pages} é¡µ"
            )

        start_idx = (current_page - 1) * items_per_page
        end_idx = start_idx + items_per_page
        page_df = processed_df.iloc[start_idx:end_idx]

        st.markdown(
            f"""
            <div style="background: #f0f2f6; padding: 12px; border-radius: 8px; margin: 10px 0;">
                ğŸ“Š å…±æ‰¾åˆ° <strong>{total_items}</strong> æ¡ç»“æœ | 
                ğŸ“„ å½“å‰æ˜¾ç¤ºç¬¬ {start_idx + 1}-{min(end_idx, total_items)} æ¡ |
                ğŸ“… æ—¶é—´è·¨åº¦ï¼š{date_range[0]} è‡³ {date_range[1]} | 
                ğŸŒŸ å¹³å‡çƒ­åº¦å€¼ï¼š{processed_df['hotness'].mean():.1f}
            </div>
            """,
            unsafe_allow_html=True
        )

        # æ˜¾ç¤ºåˆ†é¡µå¯¼èˆªæŒ‰é’®
        col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
        with col1:
            if current_page > 1:
                if st.button("â®ï¸ ç¬¬ä¸€é¡µ"):
                    st.session_state.current_page = 1
                    st.rerun()
        with col2:
            if current_page > 1:
                if st.button("â—€ï¸ ä¸Šä¸€é¡µ"):
                    st.session_state.current_page = current_page - 1
                    st.rerun()
        with col3:
            if current_page < total_pages:
                if st.button("â–¶ï¸ ä¸‹ä¸€é¡µ"):
                    st.session_state.current_page = current_page + 1
                    st.rerun()
        with col4:
            if current_page < total_pages:
                if st.button("â­ï¸ æœ€åä¸€é¡µ"):
                    st.session_state.current_page = total_pages
                    st.rerun()

        for _, row in page_df.iterrows():
            with st.container(border=True):
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    st.markdown(
                        f"""
                        <a href="{row['url']}" target="_blank" style="text-decoration: none;">
                            <h3>{row['title']}</h3>
                        </a>
                        """,
                        unsafe_allow_html=True
                    )
                    st.markdown(f"ğŸ“… **æ—¥æœŸ**: {row['date'].strftime('%Y/%m/%d')}")
                    st.markdown(f"ğŸ›ï¸ **æ¥æº**: {row['source']}")
                    st.markdown(f"ğŸŒ **åœ°åŒº**: {row['region']}")
                
                with col2:
                    st.markdown(f"ğŸ”¥ çƒ­åº¦ï¼š{row['hotness']}")
                    st.link_button("ğŸ”— æŸ¥çœ‹åŸæ–‡", row['url'], type="primary", use_container_width=True)

        # å¯¼å‡ºåŠŸèƒ½
        st.download_button(
            label="ğŸ“¥ å¯¼å‡ºå½“å‰ç»“æœï¼ˆCSVï¼‰",
            data=processed_df.to_csv(index=False).encode("utf-8"),
            file_name=f"policy_news_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            help="å¯¼å‡ºå½“å‰ç­›é€‰æ¡ä»¶ä¸‹çš„æ‰€æœ‰ç»“æœ"
        ) 